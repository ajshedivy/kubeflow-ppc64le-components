{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "changing-medication",
   "metadata": {},
   "source": [
    "# Component Test: Train Distributed with TFJob\n",
    "\n",
    "## Author\n",
    "- Sebastian Lehrig <sebastian.lehrig1@ibm.com>\n",
    "\n",
    "## License\n",
    "Apache-2.0 License"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automotive-think",
   "metadata": {},
   "source": [
    "## Imports & Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fc0794-2483-417b-9872-beaac97ef101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case packages cannot be found, do:\n",
    "# mamba uninstall transformers tokenizers\n",
    "# pip install transformers tf_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8472735-d399-41fc-bdae-e9116de11b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "from minio import Minio\n",
    "import tarfile\n",
    "from transformers import AutoTokenizer\n",
    "import yaml\n",
    "\n",
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pacific-writing",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"glue-dataset\"\n",
    "DATASET_DIR = f\"./{DATASET_NAME}\"\n",
    "DATASET_FILE = f\"{DATASET_NAME}.tar.gz\"\n",
    "MODEL_NAME = \"glue-model\"\n",
    "MODEL_FILE = f\"{MODEL_NAME}.tar.gz\"\n",
    "\n",
    "# minio-service-kubeflow.apps.b2s001.pbm.ihost.com\n",
    "MINIO_URL = \"minio-service.kubeflow:9000\"\n",
    "MINIO_USER = \"minio\"\n",
    "MINIO_PASS = \"minio123\"\n",
    "DATASETS_BUCKET = \"datasets\"\n",
    "MODELS_BUCKET = \"models\"\n",
    "\n",
    "with open(\"/var/run/secrets/kubernetes.io/serviceaccount/namespace\") as f:\n",
    "    NAMESPACE = f.read()\n",
    "NAMESPACE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intermediate-bedroom",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Client objects for interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "republican-pantyhose",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp_client = kfp.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a34102-8130-45a8-abf3-474ad930bc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "minio_client = Minio(\n",
    "    MINIO_URL, access_key=MINIO_USER, secret_key=MINIO_PASS, secure=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5657d0ab-2b0b-4bd9-8082-6d97e778de95",
   "metadata": {},
   "source": [
    "## Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839cf79d-7abe-475e-aee1-72f291acd7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"glue\", \"mrpc\", split=\"train\")\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e445a0-a741-4974-8299-682594ef764c",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb18d8ac-556d-49bf-9205-234e1b1b293f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86bd364-36e8-486a-abae-c2c75e7fea85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"sentence1\"],\n",
    "        examples[\"sentence2\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "\n",
    "dataset = dataset.map(encode, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74229811-3a4c-4b77-a8ba-cb491e973706",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda examples: {\"labels\": examples[\"label\"]}, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7beb165-e3a2-4ac1-9ffa-e58a07322bfd",
   "metadata": {},
   "source": [
    "## Save data to disk & tar.gz it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf33e86a-d592-42fa-9a8f-767d5098a467",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tarfile(output_filename, source_dir):\n",
    "    with tarfile.open(output_filename, \"w:gz\") as tar:\n",
    "        tar.add(source_dir, arcname=\".\")\n",
    "\n",
    "\n",
    "dataset.save_to_disk(DATASET_DIR)\n",
    "make_tarfile(DATASET_FILE, DATASET_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec080f79-7b14-404c-be23-57e040f1d0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {x: dataset[x] for x in [\"input_ids\", \"token_type_ids\", \"attention_mask\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4f7a88-e8cb-40d9-9291-e11218e6dd01",
   "metadata": {},
   "source": [
    "## Upload data to MinIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dce520-4caa-48ec-8374-4c12e7bf4226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets bucket if it does not yet exist\n",
    "response = minio_client.list_buckets()\n",
    "datasets_bucket_exists = False\n",
    "for bucket in response:\n",
    "    if bucket.name == DATASETS_BUCKET:\n",
    "        datasets_bucket_exists = True\n",
    "\n",
    "if not datasets_bucket_exists:\n",
    "    minio_client.make_bucket(bucket_name=DATASETS_BUCKET)\n",
    "\n",
    "minio_client.fput_object(\n",
    "    bucket_name=DATASETS_BUCKET,  # bucket name in Minio\n",
    "    object_name=DATASET_FILE,  # file name in bucket of Minio\n",
    "    file_path=DATASET_FILE,  # file path / name in local system\n",
    ")\n",
    "\n",
    "# https://kubeflow.apps.b2s001.pbm.ihost.com/pipeline/artifacts/minio/datasets/glue-dataset.tar.gz?namespace=user-example-com\n",
    "s3_address = f\"s3://{MINIO_URL}/{DATASETS_BUCKET}/{DATASET_FILE}\"\n",
    "s3_address"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d45803c-fe7d-4e03-9e96-138850458d35",
   "metadata": {},
   "source": [
    "## Specify train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0329f1-0f7e-480d-b8fa-63962dfc6ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_parameters = {\n",
    "    \"dataset_bucket\": DATASETS_BUCKET,\n",
    "    \"model_bucket\": MODELS_BUCKET,\n",
    "    \"dataset_file\": DATASET_FILE,\n",
    "    \"model_file\": MODEL_FILE,\n",
    "    \"storage_uri\": MINIO_URL,\n",
    "    \"storage_username\": MINIO_USER,\n",
    "    \"storage_password\": MINIO_PASS,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1bfabd-1ee3-4340-897b-09a94efad660",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    dataset_bucket: str,\n",
    "    model_bucket: str,\n",
    "    dataset_file: str,\n",
    "    model_file: str,\n",
    "    storage_uri: str,\n",
    "    storage_username: str,\n",
    "    storage_password: str,\n",
    "):\n",
    "    \"\"\"See https://github.com/kubeflow/training-operator/tree/master/examples/tensorflow/distribution_strategy/keras-API.\"\"\"\n",
    "\n",
    "    from datasets import load_from_disk\n",
    "    import json\n",
    "    from minio import Minio\n",
    "    import os\n",
    "    import tarfile\n",
    "    import tensorflow as tf\n",
    "    import tensorflow_datasets as tfds\n",
    "    from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "    def load_data(minio_client):\n",
    "        BUFFER_SIZE = 10000\n",
    "        DATSET_PATH = \"./dataset\"\n",
    "\n",
    "        print(f\"Fetching data from {dataset_bucket}/{dataset_file}...\")\n",
    "        data = minio_client.get_object(dataset_bucket, dataset_file)\n",
    "        with open(dataset_file, \"wb\") as file_data:\n",
    "            for d in data.stream(32 * 1024):\n",
    "                file_data.write(d)\n",
    "\n",
    "        print(f\"Extracting {dataset_file} to {DATSET_PATH}...\")\n",
    "        with tarfile.open(dataset_file, \"r:gz\") as tar_gz_ref:\n",
    "            tar_gz_ref.extractall(DATSET_PATH)\n",
    "\n",
    "        print(\"Result:\")\n",
    "        print(os.listdir(DATSET_PATH))\n",
    "\n",
    "        print(f\"Loading dataset from {DATSET_PATH}...\")\n",
    "        dataset = load_from_disk(DATSET_PATH)\n",
    "\n",
    "        print(\"Transforming to TensorFlow format...\")\n",
    "        dataset.set_format(\n",
    "            type=\"tensorflow\",\n",
    "            columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"labels\"],\n",
    "        )\n",
    "        features = {\n",
    "            x: dataset[x] for x in [\"input_ids\", \"token_type_ids\", \"attention_mask\"]\n",
    "        }\n",
    "        tf_dataset = tf.data.Dataset.from_tensor_slices((features, dataset[\"labels\"]))\n",
    "\n",
    "        return tf_dataset.cache().shuffle(BUFFER_SIZE)\n",
    "\n",
    "    def make_tarfile(output_filename, source_dir):\n",
    "        with tarfile.open(output_filename, \"w:gz\") as tar:\n",
    "            tar.add(source_dir, arcname=os.path.basename(source_dir))\n",
    "\n",
    "    def upload_data(file, minio_client):\n",
    "        print(f\"Uploading {file} to {model_bucket}/{file}...\")\n",
    "\n",
    "        # Create models bucket if it does not yet exist\n",
    "        response = minio_client.list_buckets()\n",
    "        models_bucket_exists = False\n",
    "        for bucket in response:\n",
    "            if bucket.name == model_bucket:\n",
    "                models_bucket_exists = True\n",
    "\n",
    "        if not models_bucket_exists:\n",
    "            minio_client.make_bucket(bucket_name=model_bucket)\n",
    "\n",
    "        minio_client.fput_object(\n",
    "            bucket_name=model_bucket,  # bucket name in Minio\n",
    "            object_name=file,  # file name in bucket of Minio\n",
    "            file_path=file,  # file path / name in local system\n",
    "        )\n",
    "\n",
    "    def build_and_compile_model():\n",
    "        print(\"Building model...\")\n",
    "        model = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\")\n",
    "        model.summary()\n",
    "\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "        loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "            reduction=tf.keras.losses.Reduction.NONE, from_logits=True\n",
    "        )\n",
    "        model.compile(optimizer=opt, loss=loss_fn, metrics=[\"accuracy\"])\n",
    "\n",
    "        return model\n",
    "\n",
    "    def decay(epoch):\n",
    "        if epoch < 3:\n",
    "            return 1e-3\n",
    "        if 3 <= epoch < 7:\n",
    "            return 1e-4\n",
    "        return 1e-5\n",
    "\n",
    "    # Prepare distributed training with GPU support\n",
    "    os.environ[\"NCCL_DEBUG\"] = \"INFO\"\n",
    "    tfds.disable_progress_bar()\n",
    "\n",
    "    # to decide if a worker is chief, get TASK_INDEX in Cluster info\n",
    "    tf_config = json.loads(os.environ.get(\"TF_CONFIG\") or \"{}\")\n",
    "    print(f\"tf_config: {tf_config}\")\n",
    "\n",
    "    if tf_config == json.loads(\"{}\"):\n",
    "        TASK_INDEX = 0\n",
    "    else:\n",
    "        TASK_INDEX = tf_config[\"task\"][\"index\"]\n",
    "\n",
    "    def is_chief():\n",
    "        return TASK_INDEX == 0\n",
    "\n",
    "    # MultiWorkerMirroredStrategy creates copies of all variables in the model's\n",
    "    # layers on each device across all workers\n",
    "    # if your GPUs don't support NCCL, replace \"communication\" with another\n",
    "    communication_options = tf.distribute.experimental.CommunicationOptions(\n",
    "        implementation=tf.distribute.experimental.CommunicationImplementation.RING\n",
    "    )\n",
    "    strategy = tf.distribute.MultiWorkerMirroredStrategy(\n",
    "        communication_options=communication_options\n",
    "    )\n",
    "\n",
    "    BATCH_SIZE_PER_REPLICA = 8\n",
    "    BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n",
    "    print(f\"BATCH_SIZE_PER_REPLICA: {BATCH_SIZE_PER_REPLICA}\")\n",
    "    print(f\"num_replicas_in_sync: {strategy.num_replicas_in_sync}\")\n",
    "    print(f\"BATCH_SIZE: {BATCH_SIZE}\")\n",
    "\n",
    "    with strategy.scope():\n",
    "        minio_client = Minio(\n",
    "            storage_uri,\n",
    "            access_key=storage_username,\n",
    "            secret_key=storage_password,\n",
    "            secure=False,\n",
    "        )\n",
    "        ds_train = load_data(minio_client).batch(BATCH_SIZE).repeat()\n",
    "        options = tf.data.Options()\n",
    "        options.experimental_distribute.auto_shard_policy = (\n",
    "            tf.data.experimental.AutoShardPolicy.DATA\n",
    "        )\n",
    "        ds_train = ds_train.with_options(options)\n",
    "        # Model building/compiling need to be within `strategy.scope()`.\n",
    "        multi_worker_model = build_and_compile_model()\n",
    "\n",
    "    # Checkpointing\n",
    "    checkpoint_dir = \"/train/checkpoint\"\n",
    "    checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "    # Function for decaying the learning rate.\n",
    "    # You can define any decay function you need.\n",
    "    # Callback for printing the LR at the end of each epoch.\n",
    "    class PrintLR(tf.keras.callbacks.Callback):\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            print(\n",
    "                \"\\nLearning rate for epoch {} is {}\".format(\n",
    "                    epoch + 1, multi_worker_model.optimizer.lr.numpy()\n",
    "                )\n",
    "            )\n",
    "\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.TensorBoard(log_dir=\"./logs\"),\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=checkpoint_prefix, save_weights_only=True\n",
    "        ),\n",
    "        tf.keras.callbacks.LearningRateScheduler(decay),\n",
    "        PrintLR(),\n",
    "    ]\n",
    "\n",
    "    # Keras' `model.fit()` trains the model with specified number of epochs and\n",
    "    # number of steps per epoch. Note that the numbers here are for demonstration\n",
    "    # purposes only and may not sufficiently produce a model with good quality.\n",
    "    print(\"Training model...\")\n",
    "    multi_worker_model.fit(ds_train, epochs=10, steps_per_epoch=70, callbacks=callbacks)\n",
    "\n",
    "    # Saving a model\n",
    "    saved_model_dir = \"/train/saved_model/\"\n",
    "    if is_chief():\n",
    "        model_path = saved_model_dir\n",
    "\n",
    "    else:\n",
    "        # Save to a path that is unique across workers.\n",
    "        model_path = saved_model_dir + \"/worker_tmp_\" + str(TASK_INDEX)\n",
    "\n",
    "    multi_worker_model.save(model_path)\n",
    "\n",
    "    # Upload to object store\n",
    "    if is_chief():\n",
    "        make_tarfile(model_file, model_path)\n",
    "        upload_data(model_file, minio_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419c2e39-796f-4cec-a0cc-f9c85a5e6553",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model_comp_text = kfp.components.func_to_component_text(\n",
    "    func=train_model,\n",
    "    packages_to_install=[\"transformers\", \"tf_utils\", \"tensorflow_datasets\"],\n",
    "    base_image=\"quay.io/ibm/kubeflow-notebook-image-ppc64le@sha256:97695b7b4dfab12a65b3d9aaea65649bee1769e578c0965f96648aa55f81fb27\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0083dbb8-88c0-4a7b-b052-521cbfdbf419",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model_comp_yaml = yaml.safe_load(train_model_comp_text)\n",
    "container_yaml = train_model_comp_yaml[\"implementation\"][\"container\"]\n",
    "\n",
    "image = container_yaml[\"image\"]\n",
    "command = container_yaml[\"command\"]\n",
    "args = container_yaml[\"args\"]\n",
    "for idx, arg in enumerate(args):\n",
    "    if type(arg) is dict:\n",
    "        args[idx] = train_parameters[arg[\"inputValue\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b974913f-6ea7-4c53-832c-8ac7b729f27f",
   "metadata": {},
   "source": [
    "## Load component from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840fecc8-afb1-46e6-ba35-547f0ede9d5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "distributed_train_comp = kfp.components.load_component_from_file(\"component.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nutritional-suicide",
   "metadata": {},
   "source": [
    "## Create pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "material-morning",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=\"Component Test - Train Distributed with TFJob\",\n",
    "    description=\"A simple component test\",\n",
    ")\n",
    "def train_pipeline(\n",
    "    model_name: str,\n",
    "    image: str,\n",
    "    command: list,\n",
    "    args: list,\n",
    "    namespace: str,\n",
    "):\n",
    "\n",
    "    distributed_train_comp(\n",
    "        model_name=model_name,\n",
    "        image=image,\n",
    "        command=command,\n",
    "        args=args,\n",
    "        namespace=namespace,\n",
    "        number_of_workers=\"2\",\n",
    "        pvc_size=\"10Gi\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a3bec0-144a-49d4-9c92-af5b84442231",
   "metadata": {},
   "source": [
    "## Run the pipline within an experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2cf14a-9d24-4919-ab2c-f93f99b5340d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify argument values for your pipeline run.\n",
    "arguments = {\n",
    "    \"model_name\": MODEL_NAME,\n",
    "    \"image\": image,\n",
    "    \"command\": command,\n",
    "    \"args\": args,\n",
    "    \"namespace\": NAMESPACE,\n",
    "}\n",
    "\n",
    "kfp_client.create_run_from_pipeline_func(\n",
    "    train_pipeline, arguments=arguments, namespace=NAMESPACE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98076f33-e245-49eb-ac99-b37187e5d78d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
