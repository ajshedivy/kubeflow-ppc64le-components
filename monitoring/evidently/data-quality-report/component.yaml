name: Data quality report
description: Generate a data quality report for a given dataset.
inputs:
- {name: dataset_dir, type: String, description: Path to the directory containing
    the dataset.}
- name: dataset_type
  description: |-
    Type of the dataset. Must be one of 'df' (dataframe via pickle), 'df/feather' (dataframe via feather), 'csv', or 'huggingface'.
    Defaults to 'df'.
  default: df
  optional: true
- name: ref_dataset_dir
  type: String
  description: |-
    Path to the directory containing a reference dataset for comparison.
    Defaults to None.
  optional: true
- name: additional_args
  type: JsonObject
  description: |-
    Additional arguments to be passed to the dataset processing function.
    Defaults to None.
  optional: true
- name: column_mapping
  type: JsonObject
  description: |-
    Mapping of columns between the current and reference datasets.
    Defaults to None.
  optional: true
outputs:
- {name: output_dir, type: String, description: Path to the directory where the report
    HTML file will be saved.}
- {name: mlpipeline_ui_metadata, description: Path to the file where the metadata
    for the ML Pipeline UI will be saved.}
implementation:
  container:
    image: quay.io/ibm/kubeflow-notebook-image-ppc64le:latest
    command:
    - sh
    - -c
    - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
      'evidently==0.2.6' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install
      --quiet --no-warn-script-location 'evidently==0.2.6' --user) && "$0" "$@"
    - sh
    - -ec
    - |
      program_path=$(mktemp)
      printf "%s" "$0" > "$program_path"
      python3 -u "$program_path" "$@"
    - |
      def _make_parent_dirs_and_return_path(file_path: str):
          import os
          os.makedirs(os.path.dirname(file_path), exist_ok=True)
          return file_path

      def data_quality_report(
          dataset_dir,
          output_dir,
          mlpipeline_ui_metadata_path,
          dataset_type="df",
          ref_dataset_dir = None,
          additional_args = None,
          column_mapping = None,
      ):
          """
          Generate a data quality report for a given dataset.

          Args:
              dataset_dir (str): Path to the directory containing the dataset.
              output_dir (str): Path to the directory where the report HTML file will be saved.
              mlpipeline_ui_metadata_path (str): Path to the file where the metadata for the ML Pipeline UI will be saved.
              dataset_type (str, optional): Type of the dataset. Must be one of 'df' (dataframe via pickle), 'df/feather' (dataframe via feather), 'csv', or 'huggingface'.
                  Defaults to 'df'.
              ref_dataset_dir (str, optional): Path to the directory containing a reference dataset for comparison.
                  Defaults to None.
              additional_args (dict, optional): Additional arguments to be passed to the dataset processing function.
                  Defaults to None.
              column_mapping (dict, optional): Mapping of columns between the current and reference datasets.
                  Defaults to None.

          Returns:
              None: The function saves the report HTML file and metadata to disk.

          Raises:
              KeyError: If the `dataset_type` argument is not one of 'df', 'csv', or 'huggingface'.
          """
          from datasets import load_from_disk
          from evidently.metric_preset import DataQualityPreset
          from evidently.report import Report
          import json
          import logging
          import pandas as pd
          from pathlib import Path
          import os
          import sys

          logging.basicConfig(
              stream=sys.stdout,
              level=logging.INFO,
              format="%(levelname)s %(asctime)s: %(message)s",
          )

          def _process_dataframe(dataset, opt_args=None):
              return pd.read_pickle(dataset, **opt_args)

          def _process_dataframe_feather(dataset, opt_args=None):
              return pd.read_feather(dataset, **opt_args)

          def _process_csv(dataset, opt_args=None):
              return pd.read_csv(dataset, **opt_args)

          def _process_huggingface(dataset, opt_args=None):
              return load_from_disk(dataset).to_pandas()

          DATA_TYPES = {
              "df": _process_dataframe,
              "df/feather": _process_dataframe_feather,
              "csv": _process_csv,
              "huggingface": _process_huggingface,
          }

          def process_dataset(dataset, args=None):
              _dataset_type = dataset_type.lower()

              if dataset is None:
                  return None
              if _dataset_type not in DATA_TYPES.keys():
                  raise KeyError(
                      f"Dataset type {_dataset_type} not supported by the data quality component"
                  )
              return DATA_TYPES[_dataset_type](dataset, (args or {}))

          logging.info("Preparing datasets for data quality report...")
          df = process_dataset(dataset_dir, args=additional_args)
          ref_data = process_dataset(ref_dataset_dir, args=additional_args)

          report = Report(metrics=[DataQualityPreset()])

          logging.info("Generating report using Evidently...")
          report.run(current_data=df, reference_data=ref_data, column_mapping=column_mapping)

          logging.info("Saving report as HTML...")
          Path(output_dir).parent.mkdir(parents=True, exist_ok=True)
          report.save_html(output_dir)

          logging.info("Writing HTML content to Metadata UI...")
          html_content = open(output_dir, 'r').read()
          metadata = {
              "outputs": [
                  {
                      "type": "web-app",
                      "storage": "inline",
                      "source": html_content,
                  }
              ]
          }

          with open(mlpipeline_ui_metadata_path, "w") as f:
              json.dump(metadata, f)

          logging.info("Finished.")

      import json
      import argparse
      _parser = argparse.ArgumentParser(prog='Data quality report', description='Generate a data quality report for a given dataset.')
      _parser.add_argument("--dataset-dir", dest="dataset_dir", type=str, required=True, default=argparse.SUPPRESS)
      _parser.add_argument("--dataset-type", dest="dataset_type", type=str, required=False, default=argparse.SUPPRESS)
      _parser.add_argument("--ref-dataset-dir", dest="ref_dataset_dir", type=str, required=False, default=argparse.SUPPRESS)
      _parser.add_argument("--additional-args", dest="additional_args", type=json.loads, required=False, default=argparse.SUPPRESS)
      _parser.add_argument("--column-mapping", dest="column_mapping", type=json.loads, required=False, default=argparse.SUPPRESS)
      _parser.add_argument("--output-dir", dest="output_dir", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
      _parser.add_argument("--mlpipeline-ui-metadata", dest="mlpipeline_ui_metadata_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
      _parsed_args = vars(_parser.parse_args())

      _outputs = data_quality_report(**_parsed_args)
    args:
    - --dataset-dir
    - {inputPath: dataset_dir}
    - if:
        cond: {isPresent: dataset_type}
        then:
        - --dataset-type
        - {inputValue: dataset_type}
    - if:
        cond: {isPresent: ref_dataset_dir}
        then:
        - --ref-dataset-dir
        - {inputPath: ref_dataset_dir}
    - if:
        cond: {isPresent: additional_args}
        then:
        - --additional-args
        - {inputValue: additional_args}
    - if:
        cond: {isPresent: column_mapping}
        then:
        - --column-mapping
        - {inputValue: column_mapping}
    - --output-dir
    - {outputPath: output_dir}
    - --mlpipeline-ui-metadata
    - {outputPath: mlpipeline_ui_metadata}
