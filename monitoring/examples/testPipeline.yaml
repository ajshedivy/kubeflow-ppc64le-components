apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: test-pipeline-
  annotations: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.18, pipelines.kubeflow.org/pipeline_compilation_time: '2023-01-30T16:19:28.110355',
    pipelines.kubeflow.org/pipeline_spec: '{"name": "test-pipeline"}'}
  labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.18}
spec:
  entrypoint: test-pipeline
  templates:
  - name: data-quality-report
    container:
      args: [--cur, /tmp/inputs/cur/data, --output, /tmp/outputs/output/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'evidently==0.2.0' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install
        --quiet --no-warn-script-location 'evidently==0.2.0' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def data_quality_report(
            cur_path,
            output_path,
        ):

            import pandas as pd
            from evidently.metric_preset import DataQualityPreset
            from evidently.report import Report
            from pathlib import Path

            df = pd.read_csv(cur_path)

            report = Report(metrics=[
                DataQualityPreset()
            ])

            report.run(current_data=df, reference_data=None, column_mapping=None)

            Path(output_path).parent.mkdir(parents=True, exist_ok=True)
            report.save_html(output_path)

        import argparse
        _parser = argparse.ArgumentParser(prog='Data quality report', description='')
        _parser.add_argument("--cur", dest="cur_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--output", dest="output_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = data_quality_report(**_parsed_args)
      image: quay.io/ibm/kubeflow-notebook-image-ppc64le:latest
    inputs:
      artifacts:
      - {name: download-data-output, path: /tmp/inputs/cur/data}
    outputs:
      artifacts:
      - {name: data-quality-report-output, path: /tmp/outputs/output/data}
    metadata:
      annotations: {author: Evidently AI, pipelines.kubeflow.org/component_spec: '{"implementation":
          {"container": {"args": ["--cur", {"inputPath": "cur"}, "--output", {"outputPath":
          "output"}], "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3
          -m pip install --quiet --no-warn-script-location ''evidently==0.2.0'' ||
          PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          ''evidently==0.2.0'' --user) && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef data_quality_report(\n    cur_path,\n    output_path,\n):\n\n    import
          pandas as pd\n    from evidently.metric_preset import DataQualityPreset\n    from
          evidently.report import Report\n    from pathlib import Path\n\n    df =
          pd.read_csv(cur_path)\n\n    report = Report(metrics=[\n        DataQualityPreset()\n    ])\n\n    report.run(current_data=df,
          reference_data=None, column_mapping=None)\n\n    Path(output_path).parent.mkdir(parents=True,
          exist_ok=True)\n    report.save_html(output_path)\n\nimport argparse\n_parser
          = argparse.ArgumentParser(prog=''Data quality report'', description='''')\n_parser.add_argument(\"--cur\",
          dest=\"cur_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output\",
          dest=\"output_path\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = data_quality_report(**_parsed_args)\n"], "image": "quay.io/ibm/kubeflow-notebook-image-ppc64le:latest"}},
          "inputs": [{"name": "cur", "type": "CSV"}], "metadata": {"annotations":
          {"author": "Evidently AI"}}, "name": "Data quality report", "outputs": [{"name":
          "output", "type": "HTML"}]}', pipelines.kubeflow.org/component_ref: '{"digest":
          "38ea12bd196a7a6e17d06c5aecebe9ceefa85b172c28457c15758af3b057ee33", "url":
          "/Users/adamshedivy/Documents/IBM/sandbox/kubeflow/kubeflow-ppc64le-components/monitoring/evidently/data-quality-report/component.yaml"}'}
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
  - name: download-data
    container:
      args: [--output, /tmp/outputs/output/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def download_data(
            output_path,
        ):

            from sklearn.datasets import load_breast_cancer
            from sklearn.model_selection import train_test_split
            import pandas as pd
            from pathlib import Path

            # Gets and split dataset
            data = load_breast_cancer(as_frame=True)
            X, y = data.data, data.target

            dataset = X.copy(deep=True)
            dataset['target'] = y

            Path(output_path).parent.mkdir(parents=True, exist_ok=True)
            dataset.to_csv(output_path)

        import argparse
        _parser = argparse.ArgumentParser(prog='Download data', description='')
        _parser.add_argument("--output", dest="output_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = download_data(**_parsed_args)
      image: quay.io/ibm/kubeflow-notebook-image-ppc64le:latest
    outputs:
      artifacts:
      - {name: download-data-output, path: /tmp/outputs/output/data}
    metadata:
      annotations: {author: Adam Shedivy, pipelines.kubeflow.org/component_spec: '{"implementation":
          {"container": {"args": ["--output", {"outputPath": "output"}], "command":
          ["sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef download_data(\n    output_path,\n):\n\n    from sklearn.datasets
          import load_breast_cancer\n    from sklearn.model_selection import train_test_split\n    import
          pandas as pd\n    from pathlib import Path\n\n    # Gets and split dataset\n    data
          = load_breast_cancer(as_frame=True)\n    X, y = data.data, data.target\n\n    dataset
          = X.copy(deep=True)\n    dataset[''target''] = y\n\n    Path(output_path).parent.mkdir(parents=True,
          exist_ok=True)\n    dataset.to_csv(output_path)\n\nimport argparse\n_parser
          = argparse.ArgumentParser(prog=''Download data'', description='''')\n_parser.add_argument(\"--output\",
          dest=\"output_path\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = download_data(**_parsed_args)\n"], "image": "quay.io/ibm/kubeflow-notebook-image-ppc64le:latest"}},
          "metadata": {"annotations": {"author": "Adam Shedivy"}}, "name": "Download
          data", "outputs": [{"name": "output", "type": "CSV"}]}', pipelines.kubeflow.org/component_ref: '{"digest":
          "193c4e44e8c636b0dc310f845ae5fdb10fad3e0fdf7490047db30e65d69cd6c5", "url":
          "download_breast_cancer_data.yaml"}'}
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
  - name: run-gradient-boost
    container:
      args: [--data, /tmp/inputs/data/data, --output, /tmp/outputs/output/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'scikit-learn' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
        --no-warn-script-location 'scikit-learn' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef run_gradient_boost(\n    data_path,\n    output_path,\n):\n\n    import\
        \ json\n    from pathlib import Path    \n    import pandas as pd\n    from\
        \ sklearn.ensemble import GradientBoostingClassifier\n\n    with open(data_path,\
        \ 'r') as f:\n        data = json.loads(f.read())\n\n    X_train = pd.DataFrame.from_dict(data['X_train'])\n\
        \    X_valid = pd.DataFrame.from_dict(data['X_valid'])\n    y_train = pd.DataFrame.from_dict(data['y_train'])\n\
        \    y_valid = pd.DataFrame.from_dict(data['y_valid'])\n\n    boost = GradientBoostingClassifier(\n\
        \        learning_rate=0.1,\n        n_estimators=100,\n        max_depth=8,\n\
        \        random_state=1)\n\n    boost.fit(X_train, y_train)\n\n    # print(\"\
        Training Accuracy: %0.2f\" % boost.score(X_train, y_train))\n    # print(\"\
        Validation Accuracy: %0.2f\" % boost.score(X_valid, y_valid))\n    # print(\"\
        Test Accuracy: %0.2f\" % boost.score(X_test, y_test))\n\n    Path.mkdir(output_path.parent,\
        \ parents=True, exist_ok=True)\n    with open(output_path, 'w') as f:\n  \
        \      f.write(\"Training Accuracy: %0.2f\\n\" % boost.score(X_train, y_train))\n\
        \        f.write(\"Validation Accuracy: %0.2f\\n\" % boost.score(X_valid,\
        \ y_valid))\n        f.write(\"Test Accuracy: %0.2f\\n\" % boost.score(X_test,\
        \ y_test))\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Run\
        \ gradient boost', description='')\n_parser.add_argument(\"--data\", dest=\"\
        data_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --output\", dest=\"output_path\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\
        \n_outputs = run_gradient_boost(**_parsed_args)\n"
      image: quay.io/ibm/kubeflow-notebook-image-ppc64le:latest
    inputs:
      parameters:
      - {name: train-test-split-op-output}
      artifacts:
      - name: data
        path: /tmp/inputs/data/data
        raw: {data: '{''output'': {{inputs.parameters.train-test-split-op-output}}}'}
    outputs:
      artifacts:
      - {name: run-gradient-boost-output, path: /tmp/outputs/output/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--data", {"inputPath": "data"}, "--output", {"outputPath": "output"}],
          "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip
          install --quiet --no-warn-script-location ''scikit-learn'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''scikit-learn''
          --user) && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef run_gradient_boost(\n    data_path,\n    output_path,\n):\n\n    import
          json\n    from pathlib import Path    \n    import pandas as pd\n    from
          sklearn.ensemble import GradientBoostingClassifier\n\n    with open(data_path,
          ''r'') as f:\n        data = json.loads(f.read())\n\n    X_train = pd.DataFrame.from_dict(data[''X_train''])\n    X_valid
          = pd.DataFrame.from_dict(data[''X_valid''])\n    y_train = pd.DataFrame.from_dict(data[''y_train''])\n    y_valid
          = pd.DataFrame.from_dict(data[''y_valid''])\n\n    boost = GradientBoostingClassifier(\n        learning_rate=0.1,\n        n_estimators=100,\n        max_depth=8,\n        random_state=1)\n\n    boost.fit(X_train,
          y_train)\n\n    # print(\"Training Accuracy: %0.2f\" % boost.score(X_train,
          y_train))\n    # print(\"Validation Accuracy: %0.2f\" % boost.score(X_valid,
          y_valid))\n    # print(\"Test Accuracy: %0.2f\" % boost.score(X_test, y_test))\n\n    Path.mkdir(output_path.parent,
          parents=True, exist_ok=True)\n    with open(output_path, ''w'') as f:\n        f.write(\"Training
          Accuracy: %0.2f\\n\" % boost.score(X_train, y_train))\n        f.write(\"Validation
          Accuracy: %0.2f\\n\" % boost.score(X_valid, y_valid))\n        f.write(\"Test
          Accuracy: %0.2f\\n\" % boost.score(X_test, y_test))\n\nimport argparse\n_parser
          = argparse.ArgumentParser(prog=''Run gradient boost'', description='''')\n_parser.add_argument(\"--data\",
          dest=\"data_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output\",
          dest=\"output_path\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = run_gradient_boost(**_parsed_args)\n"], "image": "quay.io/ibm/kubeflow-notebook-image-ppc64le:latest"}},
          "inputs": [{"name": "data", "type": "JSON"}], "name": "Run gradient boost",
          "outputs": [{"name": "output", "type": "String"}]}', pipelines.kubeflow.org/component_ref: '{"digest":
          "ac1181adaa6c0f04b73d46ca01822e11770de309758eac601167670c67b3f8d4", "url":
          "run_gradient_boost.yaml"}'}
  - name: test-pipeline
    dag:
      tasks:
      - name: data-quality-report
        template: data-quality-report
        dependencies: [download-data]
        arguments:
          artifacts:
          - {name: download-data-output, from: '{{tasks.download-data.outputs.artifacts.download-data-output}}'}
      - {name: download-data, template: download-data}
      - name: run-gradient-boost
        template: run-gradient-boost
        dependencies: [train-test-split-op]
        arguments:
          parameters:
          - {name: train-test-split-op-output, value: '{{tasks.train-test-split-op.outputs.parameters.train-test-split-op-output}}'}
      - name: train-test-split-op
        template: train-test-split-op
        dependencies: [download-data]
        arguments:
          artifacts:
          - {name: download-data-output, from: '{{tasks.download-data.outputs.artifacts.download-data-output}}'}
      - name: view-html
        template: view-html
        dependencies: [data-quality-report]
        arguments:
          artifacts:
          - {name: data-quality-report-output, from: '{{tasks.data-quality-report.outputs.artifacts.data-quality-report-output}}'}
  - name: train-test-split-op
    container:
      args: [--input, /tmp/inputs/input/data, --test-size, '0.2', --random-sate, '123',
        --output, /tmp/outputs/output/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'pandas' 'scikit-learn' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip
        install --quiet --no-warn-script-location 'pandas' 'scikit-learn' --user)
        && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def train_test_split_op(
            input_path,
            output_path,
            test_size = 0.2,
            random_sate = 123,
        ):

            import json
            import pandas as pd
            from pathlib import Path
            from sklearn.model_selection import train_test_split

            df = pd.read_csv(input_path)

            X, y = df.loc[:, df.columns != 'target'], df['target']

            X_temp, X_test, y_temp, y_test = \
                train_test_split(X, y, test_size=0.3, random_state=random_sate, stratify=y)

            X_train, X_valid, y_train, y_valid = \
                train_test_split(X_temp, y_temp, test_size=test_size, random_state=random_sate, stratify=y_temp)

            DATA = {
                'X_train': X_train.to_dict(),
                'X_valid': X_valid.to_dict(),
                'y_train': y_train.to_dict(),
                'y_valid': y_valid.to_dict()
            }

            json_data = json.dumps(DATA)

            Path(output_path).parent.mkdir(parents=True, exist_ok=True)
            with open(output_path, 'w') as f:
                f.write(json_data)

        import argparse
        _parser = argparse.ArgumentParser(prog='Train test split op', description='')
        _parser.add_argument("--input", dest="input_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--test-size", dest="test_size", type=float, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--random-sate", dest="random_sate", type=int, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--output", dest="output_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = train_test_split_op(**_parsed_args)
      image: quay.io/ibm/kubeflow-notebook-image-ppc64le:latest
    inputs:
      artifacts:
      - {name: download-data-output, path: /tmp/inputs/input/data}
    outputs:
      parameters:
      - name: train-test-split-op-output
        valueFrom: {path: /tmp/outputs/output/data}
      artifacts:
      - {name: train-test-split-op-output, path: /tmp/outputs/output/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--input", {"inputPath": "input"}, {"if": {"cond": {"isPresent":
          "test_size"}, "then": ["--test-size", {"inputValue": "test_size"}]}}, {"if":
          {"cond": {"isPresent": "random_sate"}, "then": ["--random-sate", {"inputValue":
          "random_sate"}]}}, "--output", {"outputPath": "output"}], "command": ["sh",
          "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          ''pandas'' ''scikit-learn'' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m
          pip install --quiet --no-warn-script-location ''pandas'' ''scikit-learn''
          --user) && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef train_test_split_op(\n    input_path,\n    output_path,\n    test_size
          = 0.2,\n    random_sate = 123,\n):\n\n    import json\n    import pandas
          as pd\n    from pathlib import Path\n    from sklearn.model_selection import
          train_test_split\n\n    df = pd.read_csv(input_path)\n\n    X, y = df.loc[:,
          df.columns != ''target''], df[''target'']\n\n    X_temp, X_test, y_temp,
          y_test = \\\n        train_test_split(X, y, test_size=0.3, random_state=random_sate,
          stratify=y)\n\n    X_train, X_valid, y_train, y_valid = \\\n        train_test_split(X_temp,
          y_temp, test_size=test_size, random_state=random_sate, stratify=y_temp)\n\n    DATA
          = {\n        ''X_train'': X_train.to_dict(),\n        ''X_valid'': X_valid.to_dict(),\n        ''y_train'':
          y_train.to_dict(),\n        ''y_valid'': y_valid.to_dict()\n    }\n\n    json_data
          = json.dumps(DATA)\n\n    Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n    with
          open(output_path, ''w'') as f:\n        f.write(json_data)\n\nimport argparse\n_parser
          = argparse.ArgumentParser(prog=''Train test split op'', description='''')\n_parser.add_argument(\"--input\",
          dest=\"input_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--test-size\",
          dest=\"test_size\", type=float, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--random-sate\",
          dest=\"random_sate\", type=int, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output\",
          dest=\"output_path\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = train_test_split_op(**_parsed_args)\n"], "image": "quay.io/ibm/kubeflow-notebook-image-ppc64le:latest"}},
          "inputs": [{"name": "input", "type": "CSV"}, {"default": "0.2", "name":
          "test_size", "optional": true, "type": "Float"}, {"default": "123", "name":
          "random_sate", "optional": true, "type": "Integer"}], "name": "Train test
          split op", "outputs": [{"name": "output", "type": "JSON"}]}', pipelines.kubeflow.org/component_ref: '{"digest":
          "4d14e7741f28f181b8739ed81c4cd06ed661dea96303847bde3617a2e53d1d93", "url":
          "train_test_split.yaml"}', pipelines.kubeflow.org/arguments.parameters: '{"random_sate":
          "123", "test_size": "0.2"}'}
  - name: view-html
    container:
      args: [--html, /tmp/inputs/html/data, --mlpipeline-ui-metadata, /tmp/outputs/mlpipeline_ui_metadata/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def view_html(
            html_path,
            mlpipeline_ui_metadata_path,
        ):

            import os
            import json

            html = os.path.abspath(html_path)
            html_content = open(html, 'r').read()

            metadata = {
                'outputs' : [{
                'type': 'web-app',
                'storage': 'inline',
                'source': html_content,
                }, {
                'type': 'web-app',
                'storage': 'inline',
                'source': '<h1>Hello, World!</h1>',
                }]
            }

            with open(mlpipeline_ui_metadata_path, 'w') as f:
                json.dump(metadata, f)

        import argparse
        _parser = argparse.ArgumentParser(prog='View html', description='')
        _parser.add_argument("--html", dest="html_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--mlpipeline-ui-metadata", dest="mlpipeline_ui_metadata_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = view_html(**_parsed_args)
      image: quay.io/ibm/kubeflow-notebook-image-ppc64le:latest
    inputs:
      artifacts:
      - {name: data-quality-report-output, path: /tmp/inputs/html/data}
    outputs:
      artifacts:
      - {name: mlpipeline-ui-metadata, path: /tmp/outputs/mlpipeline_ui_metadata/data}
    metadata:
      annotations: {author: Evidently AI, pipelines.kubeflow.org/component_spec: '{"implementation":
          {"container": {"args": ["--html", {"inputPath": "html"}, "--mlpipeline-ui-metadata",
          {"outputPath": "mlpipeline_ui_metadata"}], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef view_html(\n    html_path,\n    mlpipeline_ui_metadata_path,\n):\n\n    import
          os\n    import json\n\n    html = os.path.abspath(html_path)\n    html_content
          = open(html, ''r'').read()\n\n    metadata = {\n        ''outputs'' : [{\n        ''type'':
          ''web-app'',\n        ''storage'': ''inline'',\n        ''source'': html_content,\n        },
          {\n        ''type'': ''web-app'',\n        ''storage'': ''inline'',\n        ''source'':
          ''<h1>Hello, World!</h1>'',\n        }]\n    }\n\n    with open(mlpipeline_ui_metadata_path,
          ''w'') as f:\n        json.dump(metadata, f)\n\nimport argparse\n_parser
          = argparse.ArgumentParser(prog=''View html'', description='''')\n_parser.add_argument(\"--html\",
          dest=\"html_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--mlpipeline-ui-metadata\",
          dest=\"mlpipeline_ui_metadata_path\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = view_html(**_parsed_args)\n"], "image": "quay.io/ibm/kubeflow-notebook-image-ppc64le:latest"}},
          "inputs": [{"name": "html", "type": "HTML"}], "metadata": {"annotations":
          {"author": "Evidently AI"}}, "name": "View html", "outputs": [{"name": "mlpipeline_ui_metadata"}]}',
        pipelines.kubeflow.org/component_ref: '{"digest": "ef3c35fbce48ee454dcb3b2d65225cd31d52a73f2d38c2fb903fd0951d153223",
          "url": "/Users/adamshedivy/Documents/IBM/sandbox/kubeflow/kubeflow-ppc64le-components/monitoring/html-viewer/component.yaml"}'}
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
  arguments:
    parameters: []
  serviceAccountName: pipeline-runner
